{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook for Building Pipeline Using Module 'split_data_txt_parallel'\n",
    "The module 'split_data_txt_parallel' was generated through command: 'az ml module init -n split_data_txt_parallel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.pipeline.wrapper import Module, Pipeline, dsl\n",
    "from split_data_txt_parallel import split_data_txt_parallel"
   ]
  },
  {
   "source": [
    "## Prerequisites: Configure workspace & compute\n",
    "\n",
    " - Update config.json to reference your own workspace\n",
    " - Change the following aml_compute_target's value to reference an existing compute target"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = Workspace.from_config()\n",
    "aml_compute_target = \"YOUR_COMPUTE_TARGET\"\n",
    "print(workspace.name, workspace.resource_group, workspace.location, workspace.subscription_id, aml_compute_target, sep='\\n')"
   ]
  },
  {
   "source": [
    "# Load module(s)\n",
    "It can be done through one of the following:\n",
    "- Module.from_func to convert a python function into module function\n",
    "- Module.from_yaml to load a module function from a module yaml definition\n",
    "- Module.load to load a module function from Azure ML workspace (for registered module)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_func = Module.from_func(workspace, split_data_txt_parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run module in local\n",
    "Using module.run can execute module in local machine<br>\n",
    "If use_docker=True, will pull image from azure and run module in container.<br>\n",
    "If use_docker=False, will directly run module script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create module\n",
    "module = module_func().set_parameters(\n",
    "\n",
    ").set_inputs(\n",
    "    \n",
    ")\n",
    "module.run(use_docker=True)"
   ]
  },
  {
   "source": [
    "# Create pipeline\n",
    "This is done by calling module function(s) with the parameters/inputs/outputs the module supports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name='split_data_txt_parallel_pipeline', description='A sample pipeline uses split_data_txt_parallel', default_compute_target=aml_compute_target)\n",
    "def sample_pipeline():\n",
    "    module1 = module_func().set_parameters(\n",
    "    \n",
    "    ).set_inputs(\n",
    "\n",
    "    )\n",
    "    return module1.outputs\n",
    "\n",
    "test_pipeline = sample_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_draft = test_pipeline.save(\n",
    "    experiment_name='split_data_txt_parallel_experiment',\n",
    ")\n",
    "pipeline_draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = test_pipeline.submit(\n",
    "    experiment_name='split_data_txt_parallel_experiment'\n",
    ")\n",
    "run.wait_for_completion()\n",
    "run"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}