{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I: Using existing datasets\n",
    "If you don't have any datasets, you could use the two existing datasets we have prepared in advance. \n",
    "\n",
    "### Chinese dataset: [THUCNews](https://www.dropbox.com/sh/k9w7icz74fe4f4k/AAATZBeIh4RBu1mLZ-NqonHXa?dl=0)\n",
    "[This dataset](http://thuctc.thunlp.org/#%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86THUCNews)  is generated by filtering historical data of Sina News RSS subscription channels from 2005 to 2011. Here we only use a part of THUCNews which contains 154,921 headlines of THUCNews including nine categories: game, technology, entertainment, finance, society, realty, stock, education, and sport.\n",
    "\n",
    "### English dataset: [IMDB](https://www.dropbox.com/sh/ifxoyyt0j9kuc8u/AAD_m2q3ghJqVWEeWCWnNiyYa?dl=0)\n",
    "[This dataset](https://ai.stanford.edu/~amaas/data/sentiment/) contains 50,000 movie reviews. Each review has a label of \"pos\" or \"neg\" indicating the sentiment polarity of a user. The overall distribution of labels is balanced (25k pos and 25k neg). \n",
    "\n",
    "**Just execute the following cells, and then refer to fasttext_pipeline.ipynb, fasttext_realtime_inference.ipynb, and fasttext_batch_inference.ipynb for a further experience.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## II: Using your dataset\n",
    "**You only need to supply one file named ```data.txt```.**\n",
    "\n",
    "Each line of ```data.txt``` contains the text and label which are separated by ```\\t```. \n",
    "For example: ```sentence1``` **\\t** ```label1```\n",
    "\n",
    "**Please remove all line breaks in a text**\n",
    "\n",
    "The other files will be created automatically through the following cells.\n",
    "\n",
    "The structure of dataset files looks like:\n",
    "```\n",
    "repository\n",
    "  data\n",
    "    data_for_pipeline\n",
    "      data.txt\n",
    "      label.txt\n",
    "      word_to_index.json\n",
    "    data_for_batch_inference\n",
    "      file1\n",
    "      file2\n",
    "      ...\n",
    "      fileN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### According to the above structure, create three directories\n",
    "They are data, data_for_pipeline, and data_for_batch_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# files for fasttext_pipeline.ipynb will be placed in data/data_for_pipeline\n",
    "os.makedirs('data/data_for_pipeline', exist_ok=True)\n",
    "# files for fasttext_batch_inference.ipynb will be placed in data/data_for_batch_inference\n",
    "os.makedirs('data/data_for_batch_inference', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare files for fasttext_pipeline.ipynb\n",
    "Put ```data.txt``` in ```data/data_for_pipeline```. We will check the existence of ```data.txt```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/data_for_pipeline/data.txt'):\n",
    "    raise FileNotFoundError('data/data_for_pipeline/data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create ```label.txt```\n",
    "Each line of ```label.txt``` contains a unique label used in ```data.txt```. For example, if there are three kinds of labels in your dataset, then the ```label.txt``` looks like\n",
    "```\n",
    "label1\n",
    "label2\n",
    "label3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 9\n"
     ]
    }
   ],
   "source": [
    "labels = set()\n",
    "# get all unique labels\n",
    "with open('data/data_for_pipeline/data.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        label = line.rstrip().split('\\t')[1]\n",
    "        labels.add(label)\n",
    "labels = list(labels)\n",
    "# save labels to 'label.txt'\n",
    "with open('data/data_for_pipeline/label.txt', 'w', encoding='utf-8') as f:\n",
    "    for label in labels:\n",
    "        f.write(label+'\\n')\n",
    "print('number of labels: {}'.format(len(labels)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create ```word_to_index.json```\n",
    "This is a vocabulary file consisting of the key-value pairs. The key is a word and the value is the word index. Not all words occurred in ```data.txt```. Actually, we should take the most representative ones into account. In this notebook, we select these words according to their frequency of occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Record the word frequency\n",
    "word_count = {}\n",
    "with open('data/data_for_pipeline/data.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        text = line.split('\\t')[0]\n",
    "        for word in text.split(' '):\n",
    "            word_count[word] = word_count.get(word,0)+1\n",
    "word_count_list = sorted(word_count.items(), key=lambda x : x[1], reverse=True)\n",
    "\n",
    "# Take out the vocab_size most frequent words to form word_to_index.json\n",
    "vocab_size = 10000\n",
    "word_to_index = {\"[PAD]\": 0, \"[UNK]\": 1}\n",
    "index = 2\n",
    "for w_c in word_count_list:\n",
    "    if index == vocab_size:\n",
    "        break\n",
    "    word = w_c[0]\n",
    "    word_to_index[word] = index\n",
    "    index +=1\n",
    "with open('data/data_for_pipeline/word_to_index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_to_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate the average length of the text\n",
    "Our model needs to set a parameter named ```max_len``` which controls the length of each text. Suppose the average length of the dataset is ```avg_len```. In order to obtain a well-performed model, we need to prevent the difference between ```max_len``` and ```avg_len``` becoming too large, we suggest make ```max_len``` equal to ```avg_len```.\n",
    "\n",
    "> **Tip**\n",
    "If the variance of the text length is too large, you should use the median of the text length rather than ```avg_len```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_len:11\n"
     ]
    }
   ],
   "source": [
    "total_length = 0\n",
    "line_num = 0\n",
    "with open('data/data_for_pipeline/data.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line_num += 1\n",
    "        text = line.split('\\t')[0]\n",
    "        total_length += len(text.split(' '))\n",
    "avg_length = int(total_length/line_num)\n",
    "print('avg_len:{}'.format(avg_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare  files for fasttext_batch_inference.ipynb\n",
    "We just select a few files as the inputs for batch inference.\n",
    "\n",
    "You could determine the file names yourself, such as file1, file2, and so on.\n",
    "\n",
    "Each file contains the text you want to make a prediction. Please remove the line breaks beforehand so as to keep the text in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We select 100 files for demo\n",
    "num = 100\n",
    "with open('data/data_for_pipeline/data.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = []\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        if i==num:\n",
    "            break\n",
    "        lines.append(line.split('\\t')[0])\n",
    "       \n",
    "# Save these texts to files\n",
    "dir_ = 'data/data_for_batch_inference'\n",
    "os.makedirs(dir_, exist_ok=True)\n",
    "for index, line in enumerate(lines):\n",
    "    # We use the index as the name of each file\n",
    "    path = os.path.join(dir_, str(index))\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare files for unittest\n",
    "If you want to execute unit tests of the customized modules locally, then you need to copy related files to the specific directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copy files for customized modules: split_data_txt, fasttext_train, fasttext_evaluation, and compare_two_models\n",
    "import shutil\n",
    "src_dir = 'data/data_for_pipeline'\n",
    "dst_dir = 'split_data_txt/data/split_data_txt/inputs/input_dir'\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "for file in os.listdir(src_dir):\n",
    "    if not file.startswith('.'):\n",
    "        src = os.path.join(src_dir, file)\n",
    "        shutil.copy(src, dst_dir)\n",
    "\n",
    "# Copy files for the customized module: fasttext_score\n",
    "src_dir = 'data/data_for_batch_inference'\n",
    "dst_dir = 'fasttext_score/data/fasttext_score/inputs/input_files'\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "for file in os.listdir(src_dir):\n",
    "    if not file.startswith('.'):\n",
    "        src = os.path.join(src_dir, file)\n",
    "        shutil.copy(src, dst_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modulesdkpreview",
   "language": "python",
   "name": "modulesdkpreview"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
