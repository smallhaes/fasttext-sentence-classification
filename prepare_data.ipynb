{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "from azureml.data.datapath import DataPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create word_to_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = 'data/data.txt'\n",
    "word_to_index = {\"[PAD]\": 0, \"[UNK]\": 1}\n",
    "index = 2\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        text = line.split('\\t')[0]\n",
    "        for word in text.split(' '):\n",
    "            if not word in word_to_index:\n",
    "                word_to_index[word] = index\n",
    "                index += 1\n",
    "\n",
    "with open('data/word_to_index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_to_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare data for batch inference\n",
    "\n",
    "we need to create a file for each input sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = 'data/data.txt'\n",
    "dir_ = 'data_for_batch_inference'\n",
    "os.makedirs(dir_, exist_ok=True)\n",
    "num = 200\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    lines = []\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        if i==num:\n",
    "            break\n",
    "        lines.append(line.split('\\t')[0])\n",
    "for i, line in enumerate(lines):\n",
    "    path = os.path.join(dir_, str(i))\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='fundamental3', subscription_id='4f455bd0-f95a-4b7d-8d08-078611508e0b', resource_group='fundamental')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = Workspace.from_config('config.json')\n",
    "workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 4 files\n",
      "Uploading data/azureml/358a3e99-f299-4089-b2cf-cecc32ac34f8/Trained_model_dir/BestModel\n",
      "Uploading data/data.txt\n",
      "Uploading data/label.txt\n",
      "Uploading data/word_to_index.json\n",
      "Uploaded data/label.txt, 1 files out of an estimated total of 4\n",
      "Uploaded data/azureml/358a3e99-f299-4089-b2cf-cecc32ac34f8/Trained_model_dir/BestModel, 2 files out of an estimated total of 4\n",
      "Uploaded data/word_to_index.json, 3 files out of an estimated total of 4\n",
      "Uploaded data/data.txt, 4 files out of an estimated total of 4\n",
      "Uploaded 4 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_451c7d517a0a42188a394042cb895dfa"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_on_datastore = 'my_dataset'\n",
    "datastore = Datastore.get(workspace=workspace, datastore_name='workspaceblobstore')\n",
    "datastore.upload(src_dir='data', target_path=path_on_datastore, overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Register the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'my_dataset')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"f2d71ae3-678a-4673-9485-56c56e2e1389\",\n",
       "    \"name\": \"THUCNews\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"THUCNews dataset is generated by filtering and filtering historical data of Sina News RSS subscription channel from 2005 to 2011\",\n",
       "    \"workspace\": \"Workspace.create(name='fundamental3', subscription_id='4f455bd0-f95a-4b7d-8d08-078611508e0b', resource_group='fundamental')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'THUCNews'\n",
    "description = 'THUCNews dataset is generated by filtering and filtering historical data \\\n",
    "of Sina News RSS subscription channel from 2005 to 2011'\n",
    "datastore_path = [DataPath(datastore=datastore, path_on_datastore=path_on_datastore)]\n",
    "data = Dataset.File.from_files(path=datastore_path)\n",
    "data.register(workspace=workspace, name=dataset_name, description=description, create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 200 files\n",
      "Uploading data_for_batch_inference/0\n",
      "Uploading data_for_batch_inference/1\n",
      "Uploading data_for_batch_inference/10\n",
      "Uploading data_for_batch_inference/100\n",
      "Uploading data_for_batch_inference/101\n",
      "Uploading data_for_batch_inference/102\n",
      "Uploading data_for_batch_inference/103\n",
      "Uploading data_for_batch_inference/104\n",
      "Uploading data_for_batch_inference/105\n",
      "Uploading data_for_batch_inference/106\n",
      "Uploading data_for_batch_inference/107\n",
      "Uploading data_for_batch_inference/108\n",
      "Uploading data_for_batch_inference/109\n",
      "Uploading data_for_batch_inference/11\n",
      "Uploading data_for_batch_inference/110\n",
      "Uploading data_for_batch_inference/111\n",
      "Uploading data_for_batch_inference/112\n",
      "Uploading data_for_batch_inference/113\n",
      "Uploading data_for_batch_inference/114\n",
      "Uploading data_for_batch_inference/115\n",
      "Uploading data_for_batch_inference/116\n",
      "Uploading data_for_batch_inference/117\n",
      "Uploading data_for_batch_inference/118\n",
      "Uploading data_for_batch_inference/119\n",
      "Uploading data_for_batch_inference/12\n",
      "Uploading data_for_batch_inference/120\n",
      "Uploading data_for_batch_inference/121\n",
      "Uploading data_for_batch_inference/122\n",
      "Uploading data_for_batch_inference/123\n",
      "Uploading data_for_batch_inference/124\n",
      "Uploading data_for_batch_inference/125\n",
      "Uploading data_for_batch_inference/126\n",
      "Uploaded data_for_batch_inference/10, 1 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/127\n",
      "Uploaded data_for_batch_inference/0, 2 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/128\n",
      "Uploaded data_for_batch_inference/100, 3 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/129\n",
      "Uploaded data_for_batch_inference/105, 4 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/13\n",
      "Uploaded data_for_batch_inference/107, 5 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/130\n",
      "Uploaded data_for_batch_inference/108, 6 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/131\n",
      "Uploaded data_for_batch_inference/126, 7 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/132\n",
      "Uploaded data_for_batch_inference/11, 8 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/1, 9 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/133\n",
      "Uploaded data_for_batch_inference/129, 10 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/134\n",
      "Uploaded data_for_batch_inference/13, 11 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/135\n",
      "Uploaded data_for_batch_inference/116, 12 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/136\n",
      "Uploading data_for_batch_inference/137\n",
      "Uploaded data_for_batch_inference/127, 13 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/128, 14 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/138\n",
      "Uploaded data_for_batch_inference/114, 15 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/139\n",
      "Uploading data_for_batch_inference/14\n",
      "Uploaded data_for_batch_inference/131, 16 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/140\n",
      "Uploaded data_for_batch_inference/104, 17 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/141\n",
      "Uploaded data_for_batch_inference/101, 18 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/142\n",
      "Uploaded data_for_batch_inference/130, 19 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/143\n",
      "Uploaded data_for_batch_inference/123, 20 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/144\n",
      "Uploaded data_for_batch_inference/110, 21 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/103, 22 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/145\n",
      "Uploaded data_for_batch_inference/109, 23 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/146\n",
      "Uploaded data_for_batch_inference/121, 24 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/147\n",
      "Uploaded data_for_batch_inference/132, 25 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/148\n",
      "Uploading data_for_batch_inference/149\n",
      "Uploaded data_for_batch_inference/111, 26 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/15\n",
      "Uploaded data_for_batch_inference/113, 27 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/150\n",
      "Uploaded data_for_batch_inference/118, 28 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/120, 29 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/151\n",
      "Uploaded data_for_batch_inference/119, 30 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/152\n",
      "Uploading data_for_batch_inference/153\n",
      "Uploaded data_for_batch_inference/112, 31 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/154\n",
      "Uploaded data_for_batch_inference/106, 32 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/155\n",
      "Uploaded data_for_batch_inference/124, 33 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/156\n",
      "Uploaded data_for_batch_inference/115, 34 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/157\n",
      "Uploaded data_for_batch_inference/117, 35 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/158\n",
      "Uploaded data_for_batch_inference/139, 36 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/159\n",
      "Uploaded data_for_batch_inference/133, 37 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/16\n",
      "Uploaded data_for_batch_inference/122, 38 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/102, 39 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/160\n",
      "Uploaded data_for_batch_inference/125, 40 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/12, 41 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/161\n",
      "Uploading data_for_batch_inference/162\n",
      "Uploaded data_for_batch_inference/134, 42 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/163\n",
      "Uploaded data_for_batch_inference/141, 43 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/164\n",
      "Uploaded data_for_batch_inference/137, 44 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/165\n",
      "Uploading data_for_batch_inference/166\n",
      "Uploaded data_for_batch_inference/150, 45 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/167\n",
      "Uploaded data_for_batch_inference/152, 46 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/143, 47 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/168\n",
      "Uploaded data_for_batch_inference/155, 48 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/169\n",
      "Uploaded data_for_batch_inference/153, 49 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/17\n",
      "Uploaded data_for_batch_inference/158, 50 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/170\n",
      "Uploaded data_for_batch_inference/147, 51 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/171\n",
      "Uploaded data_for_batch_inference/145, 52 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/172\n",
      "Uploading data_for_batch_inference/173\n",
      "Uploaded data_for_batch_inference/15, 53 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/156, 54 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/174\n",
      "Uploaded data_for_batch_inference/140, 55 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/175\n",
      "Uploaded data_for_batch_inference/136, 56 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/176\n",
      "Uploaded data_for_batch_inference/138, 57 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/177\n",
      "Uploaded data_for_batch_inference/149, 58 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/178\n",
      "Uploaded data_for_batch_inference/16, 59 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/179\n",
      "Uploading data_for_batch_inference/18\n",
      "Uploaded data_for_batch_inference/146, 60 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/180\n",
      "Uploaded data_for_batch_inference/159, 61 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/181\n",
      "Uploaded data_for_batch_inference/151, 62 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/182\n",
      "Uploaded data_for_batch_inference/154, 63 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/135, 64 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/183\n",
      "Uploaded data_for_batch_inference/14, 65 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/184\n",
      "Uploading data_for_batch_inference/185\n",
      "Uploaded data_for_batch_inference/157, 66 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/186\n",
      "Uploaded data_for_batch_inference/144, 67 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/187\n",
      "Uploaded data_for_batch_inference/163, 68 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/161, 69 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/188\n",
      "Uploaded data_for_batch_inference/162, 70 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/189\n",
      "Uploading data_for_batch_inference/19\n",
      "Uploaded data_for_batch_inference/170, 71 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/190\n",
      "Uploaded data_for_batch_inference/168, 72 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/191\n",
      "Uploaded data_for_batch_inference/174, 73 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/148, 74 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/192\n",
      "Uploaded data_for_batch_inference/160, 75 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/180, 76 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/193\n",
      "Uploading data_for_batch_inference/194\n",
      "Uploaded data_for_batch_inference/182, 77 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/195\n",
      "Uploaded data_for_batch_inference/142, 78 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/196\n",
      "Uploaded data_for_batch_inference/177, 79 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/197\n",
      "Uploaded data_for_batch_inference/173, 80 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/198\n",
      "Uploaded data_for_batch_inference/172, 81 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data_for_batch_inference/171, 82 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/2\n",
      "Uploaded data_for_batch_inference/169, 83 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/20\n",
      "Uploading data_for_batch_inference/21\n",
      "Uploaded data_for_batch_inference/185, 84 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/22\n",
      "Uploaded data_for_batch_inference/18, 85 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/179, 86 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/23\n",
      "Uploading data_for_batch_inference/24\n",
      "Uploaded data_for_batch_inference/181, 87 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/25\n",
      "Uploaded data_for_batch_inference/188, 88 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/26\n",
      "Uploaded data_for_batch_inference/183, 89 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/27\n",
      "Uploaded data_for_batch_inference/178, 90 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/28\n",
      "Uploaded data_for_batch_inference/184, 91 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/29\n",
      "Uploaded data_for_batch_inference/164, 92 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/3\n",
      "Uploaded data_for_batch_inference/175, 93 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/30\n",
      "Uploaded data_for_batch_inference/186, 94 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/31\n",
      "Uploaded data_for_batch_inference/194, 95 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/32\n",
      "Uploaded data_for_batch_inference/187, 96 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/189, 97 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/33\n",
      "Uploading data_for_batch_inference/34\n",
      "Uploaded data_for_batch_inference/166, 98 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/35\n",
      "Uploaded data_for_batch_inference/17, 99 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/36\n",
      "Uploaded data_for_batch_inference/192, 100 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/37\n",
      "Uploaded data_for_batch_inference/32, 101 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/38\n",
      "Uploaded data_for_batch_inference/190, 102 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/39\n",
      "Uploaded data_for_batch_inference/28, 103 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/4\n",
      "Uploaded data_for_batch_inference/25, 104 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/21, 105 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/40\n",
      "Uploaded data_for_batch_inference/2, 106 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/41\n",
      "Uploading data_for_batch_inference/42\n",
      "Uploaded data_for_batch_inference/22, 107 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/43\n",
      "Uploaded data_for_batch_inference/23, 108 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/44\n",
      "Uploaded data_for_batch_inference/29, 109 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/167, 110 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/27, 111 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/45\n",
      "Uploading data_for_batch_inference/46\n",
      "Uploading data_for_batch_inference/47\n",
      "Uploaded data_for_batch_inference/195, 112 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/48\n",
      "Uploaded data_for_batch_inference/3, 113 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/49\n",
      "Uploaded data_for_batch_inference/165, 114 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/5\n",
      "Uploaded data_for_batch_inference/35, 115 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/50\n",
      "Uploaded data_for_batch_inference/30, 116 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/51\n",
      "Uploaded data_for_batch_inference/24, 117 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/52\n",
      "Uploaded data_for_batch_inference/176, 118 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/53\n",
      "Uploaded data_for_batch_inference/41, 119 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/42, 120 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/54\n",
      "Uploaded data_for_batch_inference/197, 121 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/55\n",
      "Uploading data_for_batch_inference/56\n",
      "Uploaded data_for_batch_inference/38, 122 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/34, 123 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/57\n",
      "Uploaded data_for_batch_inference/33, 124 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/58\n",
      "Uploaded data_for_batch_inference/4, 125 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/59\n",
      "Uploaded data_for_batch_inference/191, 126 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/37, 127 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/6\n",
      "Uploading data_for_batch_inference/60\n",
      "Uploading data_for_batch_inference/61\n",
      "Uploaded data_for_batch_inference/39, 128 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/193, 129 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/62\n",
      "Uploading data_for_batch_inference/63\n",
      "Uploaded data_for_batch_inference/31, 130 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/64\n",
      "Uploaded data_for_batch_inference/56, 131 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/199, 132 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/65\n",
      "Uploaded data_for_batch_inference/6, 133 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/66\n",
      "Uploaded data_for_batch_inference/26, 134 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/67\n",
      "Uploaded data_for_batch_inference/43, 135 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/68\n",
      "Uploading data_for_batch_inference/69\n",
      "Uploaded data_for_batch_inference/46, 136 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/7\n",
      "Uploaded data_for_batch_inference/57, 137 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/70\n",
      "Uploaded data_for_batch_inference/44, 138 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/71\n",
      "Uploaded data_for_batch_inference/55, 139 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/51, 140 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/72\n",
      "Uploaded data_for_batch_inference/36, 141 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/73\n",
      "Uploaded data_for_batch_inference/58, 142 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/74\n",
      "Uploading data_for_batch_inference/75\n",
      "Uploaded data_for_batch_inference/53, 143 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/61, 144 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/76\n",
      "Uploaded data_for_batch_inference/196, 145 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/77\n",
      "Uploaded data_for_batch_inference/19, 146 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/78\n",
      "Uploaded data_for_batch_inference/60, 147 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/79\n",
      "Uploading data_for_batch_inference/8\n",
      "Uploaded data_for_batch_inference/54, 148 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/59, 149 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/80\n",
      "Uploaded data_for_batch_inference/198, 150 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/81\n",
      "Uploaded data_for_batch_inference/40, 151 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/82\n",
      "Uploading data_for_batch_inference/83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data_for_batch_inference/20, 152 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/84\n",
      "Uploaded data_for_batch_inference/69, 153 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/85\n",
      "Uploaded data_for_batch_inference/65, 154 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/86\n",
      "Uploaded data_for_batch_inference/66, 155 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/87\n",
      "Uploaded data_for_batch_inference/71, 156 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/88\n",
      "Uploaded data_for_batch_inference/7, 157 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/89\n",
      "Uploaded data_for_batch_inference/45, 158 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/9\n",
      "Uploaded data_for_batch_inference/68, 159 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/90\n",
      "Uploaded data_for_batch_inference/70, 160 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/91\n",
      "Uploaded data_for_batch_inference/64, 161 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/92\n",
      "Uploaded data_for_batch_inference/86, 162 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/72, 163 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/93\n",
      "Uploading data_for_batch_inference/94\n",
      "Uploaded data_for_batch_inference/73, 164 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/95\n",
      "Uploaded data_for_batch_inference/52, 165 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/96\n",
      "Uploaded data_for_batch_inference/77, 166 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/97\n",
      "Uploaded data_for_batch_inference/48, 167 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/98\n",
      "Uploaded data_for_batch_inference/89, 168 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/75, 169 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/50, 170 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/99\n",
      "Uploaded data_for_batch_inference/91, 171 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/76, 172 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/88, 173 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/9, 174 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/83, 175 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/63, 176 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/62, 177 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/87, 178 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/92, 179 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/80, 180 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/94, 181 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/90, 182 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/93, 183 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/78, 184 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/47, 185 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/82, 186 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/67, 187 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/84, 188 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/95, 189 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/74, 190 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/79, 191 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/98, 192 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/97, 193 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/5, 194 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/96, 195 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/99, 196 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/8, 197 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/49, 198 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/81, 199 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/85, 200 files out of an estimated total of 200\n",
      "Uploaded 200 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_b09637c7feff45f793275fa2a9123854"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_on_datastore = 'my_dataset_for_batch_inference'\n",
    "datastore = Datastore.get(workspace=workspace, datastore_name='workspaceblobstore')\n",
    "datastore.upload(src_dir='data_for_batch_inference', target_path=path_on_datastore, overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Register the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'my_dataset_for_batch_inference')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"f519240d-08f3-44bb-8e79-58b8c90810d6\",\n",
       "    \"name\": \"THUCNews_For_Batch_Inference\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"THUCNews dataset is generated by filtering and filtering historical data of Sina News RSS subscription channel from 2005 to 2011\",\n",
       "    \"workspace\": \"Workspace.create(name='fundamental3', subscription_id='4f455bd0-f95a-4b7d-8d08-078611508e0b', resource_group='fundamental')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'THUCNews_For_Batch_Inference'\n",
    "description = 'THUCNews dataset is generated by filtering and filtering historical data \\\n",
    "of Sina News RSS subscription channel from 2005 to 2011'\n",
    "datastore_path = [DataPath(datastore=datastore, path_on_datastore=path_on_datastore)]\n",
    "data = Dataset.File.from_files(path=datastore_path)\n",
    "data.register(workspace=workspace, name=dataset_name, description=description, create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
