{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "from azureml.data.datapath import DataPath"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create word_to_index.json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "path = 'data/data.txt'\n",
    "word_to_index = {\"[PAD]\": 0, \"[UNK]\": 1}\n",
    "index = 2\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        text = line.split('\\t')[0]\n",
    "        for word in text.split(' '):\n",
    "            if not word in word_to_index:\n",
    "                word_to_index[word] = index\n",
    "                index += 1\n",
    "\n",
    "with open('data/word_to_index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_to_index, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare data for batch inference\n",
    "\n",
    "we need to create a file for each input sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dir_ = 'data_for_batch_inference'\n",
    "os.makedirs(dir_, exist_ok=True)\n",
    "num = 200\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    lines = []\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        if i==num:\n",
    "            break\n",
    "        lines.append(line.split('\\t')[0])\n",
    "for i, line in enumerate(lines):\n",
    "    path = os.path.join(dir_, str(i))\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare your workspace"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Workspace.create(name='fundamental3', subscription_id='4f455bd0-f95a-4b7d-8d08-078611508e0b', resource_group='fundamental')"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = Workspace.from_config('config.json')\n",
    "workspace"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### datastore"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 5 files\n",
      "Uploading data\\char_to_index.json\n",
      "Uploading data\\data.txt\n",
      "Uploading data\\data_origin.txt\n",
      "Uploading data\\label.txt\n",
      "Uploading data\\word_to_index.json\n",
      "Uploaded data\\label.txt, 1 files out of an estimated total of 5\n",
      "Uploaded data\\char_to_index.json, 2 files out of an estimated total of 5\n",
      "Uploaded data\\word_to_index.json, 3 files out of an estimated total of 5\n",
      "Uploaded 3 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 677, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 392, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\http\\client.py\", line 1239, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\http\\client.py\", line 1285, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\http\\client.py\", line 1234, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\http\\client.py\", line 1065, in _send_output\n",
      "    self.send(chunk)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\http\\client.py\", line 986, in send\n",
      "    self.sock.sendall(data)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\ssl.py\", line 975, in sendall\n",
      "    v = self.send(byte_view[count:])\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\ssl.py\", line 944, in send\n",
      "    return self._sslobj.write(data)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\ssl.py\", line 642, in write\n",
      "    return self._sslobj.write(data)\n",
      "socket.timeout: The write operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 765, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 765, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 765, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 725, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\util\\retry.py\", line 439, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='fundamental35269596308.blob.core.windows.net', port=443): Max retries exceeded with url: /azureml-blobstore-a7c2acaa-c8f1-448b-b894-c2e7a71b3a32/my_dataset%5Cdata_origin.txt (Caused by ProtocolError('Connection aborted.', timeout('The write operation timed out',)))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\common\\storageclient.py\", line 269, in _perform_request\n",
      "    response = self._httpclient.perform_request(request)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\common\\_http\\httpclient.py\", line 92, in perform_request\n",
      "    proxies=self.proxies)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\requests\\sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\requests\\sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='fundamental35269596308.blob.core.windows.net', port=443): Max retries exceeded with url: /azureml-blobstore-a7c2acaa-c8f1-448b-b894-c2e7a71b3a32/my_dataset%5Cdata_origin.txt (Caused by ProtocolError('Connection aborted.', timeout('The write operation timed out',)))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\data\\azure_storage_datastore.py\", line 331, in handler\n",
      "    result = fn()\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\data\\azure_storage_datastore.py\", line 739, in <lambda>\n",
      "    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\blob\\blockblobservice.py\", line 463, in create_blob_from_path\n",
      "    timeout=timeout)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\blob\\blockblobservice.py\", line 582, in create_blob_from_stream\n",
      "    timeout=timeout)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\blob\\blockblobservice.py\", line 971, in _put_blob\n",
      "    return self._perform_request(request, _parse_base_properties)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\common\\storageclient.py\", line 381, in _perform_request\n",
      "    raise ex\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\common\\storageclient.py\", line 311, in _perform_request\n",
      "    raise AzureException(ex.args[0])\n",
      "azure.common.AzureException: HTTPSConnectionPool(host='fundamental35269596308.blob.core.windows.net', port=443): Max retries exceeded with url: /azureml-blobstore-a7c2acaa-c8f1-448b-b894-c2e7a71b3a32/my_dataset%5Cdata_origin.txt (Caused by ProtocolError('Connection aborted.', timeout('The write operation timed out',)))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\logging\\__init__.py\", line 994, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\logging\\__init__.py\", line 840, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\logging\\__init__.py\", line 577, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\logging\\__init__.py\", line 338, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\threading.py\", line 884, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_history\\utils\\task_queue.py\", line 57, in _awaiter\n",
      "    result = task.wait()\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_history\\utils\\async_task.py\", line 48, in wait\n",
      "    return self._handler(self._task, self._logger)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\data\\azure_storage_datastore.py\", line 339, in handler\n",
      "    exception_handler(e, logger)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\data\\azure_storage_datastore.py\", line 303, in exception_handler\n",
      "    logger.error(\"Upload failed, please make sure target_path does not start with invalid characters.\", e)\n",
      "Message: 'Upload failed, please make sure target_path does not start with invalid characters.'\n",
      "Arguments: (AzureException(MaxRetryError(\"HTTPSConnectionPool(host='fundamental35269596308.blob.core.windows.net', port=443): Max retries exceeded with url: /azureml-blobstore-a7c2acaa-c8f1-448b-b894-c2e7a71b3a32/my_dataset%5Cdata_origin.txt (Caused by ProtocolError('Connection aborted.', timeout('The write operation timed out',)))\",),),)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 677, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 392, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\http\\client.py\", line 1239, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\http\\client.py\", line 1285, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\http\\client.py\", line 1234, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\http\\client.py\", line 1065, in _send_output\n",
      "    self.send(chunk)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\http\\client.py\", line 986, in send\n",
      "    self.sock.sendall(data)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\ssl.py\", line 975, in sendall\n",
      "    v = self.send(byte_view[count:])\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\ssl.py\", line 944, in send\n",
      "    return self._sslobj.write(data)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\ssl.py\", line 642, in write\n",
      "    return self._sslobj.write(data)\n",
      "socket.timeout: The write operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 765, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 765, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 765, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 725, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\urllib3\\util\\retry.py\", line 439, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='fundamental35269596308.blob.core.windows.net', port=443): Max retries exceeded with url: /azureml-blobstore-a7c2acaa-c8f1-448b-b894-c2e7a71b3a32/my_dataset%5Cdata.txt (Caused by ProtocolError('Connection aborted.', timeout('The write operation timed out',)))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\common\\storageclient.py\", line 269, in _perform_request\n",
      "    response = self._httpclient.perform_request(request)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\common\\_http\\httpclient.py\", line 92, in perform_request\n",
      "    proxies=self.proxies)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\requests\\sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\requests\\sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='fundamental35269596308.blob.core.windows.net', port=443): Max retries exceeded with url: /azureml-blobstore-a7c2acaa-c8f1-448b-b894-c2e7a71b3a32/my_dataset%5Cdata.txt (Caused by ProtocolError('Connection aborted.', timeout('The write operation timed out',)))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\data\\azure_storage_datastore.py\", line 331, in handler\n",
      "    result = fn()\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\data\\azure_storage_datastore.py\", line 739, in <lambda>\n",
      "    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\blob\\blockblobservice.py\", line 463, in create_blob_from_path\n",
      "    timeout=timeout)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\blob\\blockblobservice.py\", line 582, in create_blob_from_stream\n",
      "    timeout=timeout)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\blob\\blockblobservice.py\", line 971, in _put_blob\n",
      "    return self._perform_request(request, _parse_base_properties)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\common\\storageclient.py\", line 381, in _perform_request\n",
      "    raise ex\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_vendor\\azure_storage\\common\\storageclient.py\", line 311, in _perform_request\n",
      "    raise AzureException(ex.args[0])\n",
      "azure.common.AzureException: HTTPSConnectionPool(host='fundamental35269596308.blob.core.windows.net', port=443): Max retries exceeded with url: /azureml-blobstore-a7c2acaa-c8f1-448b-b894-c2e7a71b3a32/my_dataset%5Cdata.txt (Caused by ProtocolError('Connection aborted.', timeout('The write operation timed out',)))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\logging\\__init__.py\", line 994, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\logging\\__init__.py\", line 840, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\logging\\__init__.py\", line 577, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\logging\\__init__.py\", line 338, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\threading.py\", line 884, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_history\\utils\\task_queue.py\", line 57, in _awaiter\n",
      "    result = task.wait()\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\_history\\utils\\async_task.py\", line 48, in wait\n",
      "    return self._handler(self._task, self._logger)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\data\\azure_storage_datastore.py\", line 339, in handler\n",
      "    exception_handler(e, logger)\n",
      "  File \"C:\\Users\\50657\\miniconda3\\envs\\tmp2\\lib\\site-packages\\azureml\\data\\azure_storage_datastore.py\", line 303, in exception_handler\n",
      "    logger.error(\"Upload failed, please make sure target_path does not start with invalid characters.\", e)\n",
      "Message: 'Upload failed, please make sure target_path does not start with invalid characters.'\n",
      "Arguments: (AzureException(MaxRetryError(\"HTTPSConnectionPool(host='fundamental35269596308.blob.core.windows.net', port=443): Max retries exceeded with url: /azureml-blobstore-a7c2acaa-c8f1-448b-b894-c2e7a71b3a32/my_dataset%5Cdata.txt (Caused by ProtocolError('Connection aborted.', timeout('The write operation timed out',)))\",),),)\n"
     ]
    },
    {
     "data": {
      "text/plain": "$AZUREML_DATAREFERENCE_d86a6dd4621144198fcf2d7be8847eb9"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_on_datastore = 'my_dataset'\n",
    "datastore = Datastore.get(workspace=workspace, datastore_name='workspaceblobstore')\n",
    "datastore.upload(src_dir='data', target_path=path_on_datastore, overwrite=True, show_progress=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### register\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "{\n  \"source\": [\n    \"('workspaceblobstore', 'my_dataset')\"\n  ],\n  \"definition\": [\n    \"GetDatastoreFiles\"\n  ],\n  \"registration\": {\n    \"id\": \"0c7ae634-ad70-40d5-88cb-9e40c3572999\",\n    \"name\": \"THUCNews\",\n    \"version\": 2,\n    \"description\": \"THUCNews dataset is generated by filtering and filtering historical data of Sina News RSS subscription channel from 2005 to 2011\",\n    \"workspace\": \"Workspace.create(name='fundamental3', subscription_id='4f455bd0-f95a-4b7d-8d08-078611508e0b', resource_group='fundamental')\"\n  }\n}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'THUCNews'\n",
    "description = 'THUCNews dataset is generated by filtering and filtering historical data \\\n",
    "of Sina News RSS subscription channel from 2005 to 2011'\n",
    "datastore_path = [DataPath(datastore=datastore, path_on_datastore=path_on_datastore)]\n",
    "data = Dataset.File.from_files(path=datastore_path)\n",
    "data.register(workspace=workspace, name=dataset_name, description=description, create_new_version=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### datastore"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 200 files\n",
      "Uploading data_for_batch_inference\\0\n",
      "Uploading data_for_batch_inference\\1\n",
      "Uploading data_for_batch_inference\\10\n",
      "Uploading data_for_batch_inference\\100\n",
      "Uploading data_for_batch_inference\\101\n",
      "Uploading data_for_batch_inference\\102\n",
      "Uploading data_for_batch_inference\\103\n",
      "Uploading data_for_batch_inference\\104\n",
      "Uploading data_for_batch_inference\\105\n",
      "Uploading data_for_batch_inference\\106\n",
      "Uploading data_for_batch_inference\\107\n",
      "Uploading data_for_batch_inference\\108\n",
      "Uploading data_for_batch_inference\\109\n",
      "Uploading data_for_batch_inference\\11\n",
      "Uploading data_for_batch_inference\\110\n",
      "Uploading data_for_batch_inference\\111\n",
      "Uploading data_for_batch_inference\\112\n",
      "Uploading data_for_batch_inference\\113\n",
      "Uploading data_for_batch_inference\\114\n",
      "Uploading data_for_batch_inference\\115\n",
      "Uploading data_for_batch_inference\\116\n",
      "Uploading data_for_batch_inference\\117\n",
      "Uploading data_for_batch_inference\\118\n",
      "Uploading data_for_batch_inference\\119\n",
      "Uploading data_for_batch_inference\\12\n",
      "Uploading data_for_batch_inference\\120\n",
      "Uploading data_for_batch_inference\\121\n",
      "Uploading data_for_batch_inference\\122\n",
      "Uploading data_for_batch_inference\\123\n",
      "Uploading data_for_batch_inference\\124\n",
      "Uploading data_for_batch_inference\\125\n",
      "Uploading data_for_batch_inference\\126\n",
      "Uploaded data_for_batch_inference\\1, 1 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\127\n",
      "Uploaded data_for_batch_inference\\108, 2 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\128\n",
      "Uploaded data_for_batch_inference\\127, 3 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\129\n",
      "Uploaded data_for_batch_inference\\107, 4 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\13\n",
      "Uploaded data_for_batch_inference\\102, 5 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\130\n",
      "Uploaded data_for_batch_inference\\128, 6 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\131\n",
      "Uploaded data_for_batch_inference\\129, 7 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\132\n",
      "Uploaded data_for_batch_inference\\13, 8 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\133\n",
      "Uploaded data_for_batch_inference\\119, 9 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\130, 10 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\134\n",
      "Uploaded data_for_batch_inference\\131, 11 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\135\n",
      "Uploading data_for_batch_inference\\136\n",
      "Uploaded data_for_batch_inference\\132, 12 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\137\n",
      "Uploaded data_for_batch_inference\\133, 13 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\138\n",
      "Uploaded data_for_batch_inference\\114, 14 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\139\n",
      "Uploaded data_for_batch_inference\\136, 15 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\14\n",
      "Uploaded data_for_batch_inference\\134, 16 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\135, 17 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\140\n",
      "Uploading data_for_batch_inference\\141\n",
      "Uploaded data_for_batch_inference\\137, 18 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\142\n",
      "Uploaded data_for_batch_inference\\138, 19 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\143\n",
      "Uploaded data_for_batch_inference\\11, 20 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\144\n",
      "Uploaded data_for_batch_inference\\139, 21 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\145\n",
      "Uploaded data_for_batch_inference\\142, 22 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\14, 23 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\141, 24 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\140, 25 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\146\n",
      "Uploading data_for_batch_inference\\147\n",
      "Uploading data_for_batch_inference\\148\n",
      "Uploading data_for_batch_inference\\149\n",
      "Uploaded data_for_batch_inference\\110, 26 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\15\n",
      "Uploaded data_for_batch_inference\\143, 27 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\150\n",
      "Uploaded data_for_batch_inference\\144, 28 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\151\n",
      "Uploaded data_for_batch_inference\\145, 29 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\152\n",
      "Uploaded data_for_batch_inference\\12, 30 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\153\n",
      "Uploaded data_for_batch_inference\\15, 31 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\147, 32 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\148, 33 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\154\n",
      "Uploaded data_for_batch_inference\\146, 34 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\155\n",
      "Uploading data_for_batch_inference\\156\n",
      "Uploaded data_for_batch_inference\\149, 35 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\157\n",
      "Uploaded data_for_batch_inference\\150, 36 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\158\n",
      "Uploading data_for_batch_inference\\159\n",
      "Uploaded data_for_batch_inference\\151, 37 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\16\n",
      "Uploaded data_for_batch_inference\\10, 38 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\152, 39 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\160\n",
      "Uploading data_for_batch_inference\\161\n",
      "Uploaded data_for_batch_inference\\153, 40 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\162\n",
      "Uploaded data_for_batch_inference\\120, 41 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\163\n",
      "Uploaded data_for_batch_inference\\156, 42 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\164\n",
      "Uploaded data_for_batch_inference\\157, 43 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\158, 44 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\154, 45 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\165\n",
      "Uploading data_for_batch_inference\\166\n",
      "Uploaded data_for_batch_inference\\155, 46 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\167\n",
      "Uploaded data_for_batch_inference\\159, 47 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\168\n",
      "Uploading data_for_batch_inference\\169\n",
      "Uploaded data_for_batch_inference\\16, 48 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\17\n",
      "Uploaded data_for_batch_inference\\160, 49 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\170\n",
      "Uploaded data_for_batch_inference\\161, 50 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\171\n",
      "Uploaded data_for_batch_inference\\112, 51 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\162, 52 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\172\n",
      "Uploading data_for_batch_inference\\173\n",
      "Uploaded data_for_batch_inference\\166, 53 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\163, 54 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\174\n",
      "Uploaded data_for_batch_inference\\164, 55 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\167, 56 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\17, 57 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\175\n",
      "Uploaded data_for_batch_inference\\170, 58 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\176\n",
      "Uploaded data_for_batch_inference\\165, 59 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\177\n",
      "Uploaded data_for_batch_inference\\122, 60 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\178\n",
      "Uploading data_for_batch_inference\\179\n",
      "Uploaded data_for_batch_inference\\168, 61 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\18\n",
      "Uploading data_for_batch_inference\\180\n",
      "Uploaded data_for_batch_inference\\171, 62 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\169, 63 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\181\n",
      "Uploaded data_for_batch_inference\\173, 64 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\182\n",
      "Uploading data_for_batch_inference\\183\n",
      "Uploading data_for_batch_inference\\184\n",
      "Uploaded data_for_batch_inference\\172, 65 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\185\n",
      "Uploaded data_for_batch_inference\\113, 66 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\186\n",
      "Uploaded data_for_batch_inference\\180, 67 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\177, 68 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\174, 69 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\187\n",
      "Uploading data_for_batch_inference\\188\n",
      "Uploaded data_for_batch_inference\\183, 70 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\189\n",
      "Uploading data_for_batch_inference\\19\n",
      "Uploaded data_for_batch_inference\\181, 71 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\18, 72 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\190\n",
      "Uploaded data_for_batch_inference\\179, 73 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\191\n",
      "Uploaded data_for_batch_inference\\182, 74 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\184, 75 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\178, 76 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\192\n",
      "Uploaded data_for_batch_inference\\185, 77 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\176, 78 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\193\n",
      "Uploaded data_for_batch_inference\\175, 79 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\194\n",
      "Uploading data_for_batch_inference\\195\n",
      "Uploading data_for_batch_inference\\196\n",
      "Uploading data_for_batch_inference\\197\n",
      "Uploading data_for_batch_inference\\198\n",
      "Uploaded data_for_batch_inference\\105, 80 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\199\n",
      "Uploaded data_for_batch_inference\\101, 81 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\2\n",
      "Uploaded data_for_batch_inference\\186, 82 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\20\n",
      "Uploaded data_for_batch_inference\\188, 83 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\21\n",
      "Uploaded data_for_batch_inference\\193, 84 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\187, 85 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\22\n",
      "Uploaded data_for_batch_inference\\190, 86 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\23\n",
      "Uploading data_for_batch_inference\\24\n",
      "Uploaded data_for_batch_inference\\196, 87 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\25\n",
      "Uploaded data_for_batch_inference\\194, 88 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\195, 89 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\197, 90 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\19, 91 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\26\n",
      "Uploading data_for_batch_inference\\27\n",
      "Uploading data_for_batch_inference\\28\n",
      "Uploaded data_for_batch_inference\\192, 92 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\29\n",
      "Uploaded data_for_batch_inference\\198, 93 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\3\n",
      "Uploading data_for_batch_inference\\30\n",
      "Uploaded data_for_batch_inference\\189, 94 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\191, 95 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\31\n",
      "Uploading data_for_batch_inference\\32\n",
      "Uploaded data_for_batch_inference\\199, 96 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\33\n",
      "Uploaded data_for_batch_inference\\2, 97 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\34\n",
      "Uploaded data_for_batch_inference\\106, 98 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\35\n",
      "Uploaded data_for_batch_inference\\20, 99 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\36\n",
      "Uploaded data_for_batch_inference\\21, 100 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\37\n",
      "Uploaded data_for_batch_inference\\23, 101 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\22, 102 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\38\n",
      "Uploaded data_for_batch_inference\\25, 103 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\39\n",
      "Uploading data_for_batch_inference\\4\n",
      "Uploaded data_for_batch_inference\\24, 104 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\40\n",
      "Uploaded data_for_batch_inference\\3, 105 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\27, 106 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\41\n",
      "Uploaded data_for_batch_inference\\31, 107 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\26, 108 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\29, 109 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\42\n",
      "Uploading data_for_batch_inference\\43\n",
      "Uploaded data_for_batch_inference\\30, 110 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\32, 111 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\44\n",
      "Uploading data_for_batch_inference\\45\n",
      "Uploading data_for_batch_inference\\46\n",
      "Uploading data_for_batch_inference\\47\n",
      "Uploaded data_for_batch_inference\\28, 112 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\118, 113 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\48\n",
      "Uploading data_for_batch_inference\\49\n",
      "Uploaded data_for_batch_inference\\34, 114 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\33, 115 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\5\n",
      "Uploading data_for_batch_inference\\50\n",
      "Uploaded data_for_batch_inference\\35, 116 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\51\n",
      "Uploaded data_for_batch_inference\\36, 117 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\52\n",
      "Uploaded data_for_batch_inference\\37, 118 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\53\n",
      "Uploaded data_for_batch_inference\\4, 119 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\40, 120 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\38, 121 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\54\n",
      "Uploading data_for_batch_inference\\55\n",
      "Uploading data_for_batch_inference\\56\n",
      "Uploaded data_for_batch_inference\\39, 122 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\57\n",
      "Uploaded data_for_batch_inference\\43, 123 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\58\n",
      "Uploaded data_for_batch_inference\\100, 124 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\41, 125 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\59\n",
      "Uploading data_for_batch_inference\\6\n",
      "Uploaded data_for_batch_inference\\42, 126 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\47, 127 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\60\n",
      "Uploading data_for_batch_inference\\61\n",
      "Uploaded data_for_batch_inference\\46, 128 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\44, 129 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\62\n",
      "Uploading data_for_batch_inference\\63\n",
      "Uploaded data_for_batch_inference\\5, 130 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\64\n",
      "Uploaded data_for_batch_inference\\49, 131 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\50, 132 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\65\n",
      "Uploaded data_for_batch_inference\\45, 133 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\66\n",
      "Uploaded data_for_batch_inference\\48, 134 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\67\n",
      "Uploading data_for_batch_inference\\68\n",
      "Uploaded data_for_batch_inference\\116, 135 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\69\n",
      "Uploaded data_for_batch_inference\\51, 136 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\7\n",
      "Uploaded data_for_batch_inference\\52, 137 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\70\n",
      "Uploaded data_for_batch_inference\\53, 138 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\71\n",
      "Uploaded data_for_batch_inference\\54, 139 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\57, 140 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\56, 141 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\72\n",
      "Uploading data_for_batch_inference\\73\n",
      "Uploading data_for_batch_inference\\74\n",
      "Uploaded data_for_batch_inference\\55, 142 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\75\n",
      "Uploaded data_for_batch_inference\\58, 143 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\76\n",
      "Uploaded data_for_batch_inference\\6, 144 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\63, 145 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\77\n",
      "Uploading data_for_batch_inference\\78\n",
      "Uploaded data_for_batch_inference\\59, 146 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\79\n",
      "Uploaded data_for_batch_inference\\64, 147 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\8\n",
      "Uploaded data_for_batch_inference\\109, 148 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\80\n",
      "Uploaded data_for_batch_inference\\61, 149 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\81\n",
      "Uploaded data_for_batch_inference\\62, 150 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\60, 151 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\65, 152 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\82\n",
      "Uploading data_for_batch_inference\\83\n",
      "Uploaded data_for_batch_inference\\7, 153 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\67, 154 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\84\n",
      "Uploaded data_for_batch_inference\\66, 155 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\68, 156 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\85\n",
      "Uploaded data_for_batch_inference\\70, 157 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\86\n",
      "Uploading data_for_batch_inference\\87\n",
      "Uploading data_for_batch_inference\\88\n",
      "Uploading data_for_batch_inference\\89\n",
      "Uploaded data_for_batch_inference\\69, 158 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\9\n",
      "Uploaded data_for_batch_inference\\71, 159 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\90\n",
      "Uploaded data_for_batch_inference\\73, 160 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\75, 161 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\91\n",
      "Uploading data_for_batch_inference\\92\n",
      "Uploaded data_for_batch_inference\\74, 162 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\93\n",
      "Uploaded data_for_batch_inference\\72, 163 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\94\n",
      "Uploaded data_for_batch_inference\\104, 164 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\95\n",
      "Uploaded data_for_batch_inference\\76, 165 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\96\n",
      "Uploaded data_for_batch_inference\\77, 166 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\97\n",
      "Uploaded data_for_batch_inference\\81, 167 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\98\n",
      "Uploaded data_for_batch_inference\\78, 168 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\84, 169 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference\\99\n",
      "Uploaded data_for_batch_inference\\83, 170 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\85, 171 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\88, 172 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\86, 173 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\89, 174 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\8, 175 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\79, 176 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\82, 177 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\90, 178 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\9, 179 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\103, 180 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\80, 181 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\87, 182 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\91, 183 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\92, 184 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\94, 185 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\93, 186 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\96, 187 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\95, 188 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\97, 189 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\124, 190 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\98, 191 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\99, 192 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\125, 193 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\121, 194 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\126, 195 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\123, 196 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\115, 197 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\0, 198 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\117, 199 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference\\111, 200 files out of an estimated total of 200\n",
      "Uploaded 200 files\n"
     ]
    },
    {
     "data": {
      "text/plain": "$AZUREML_DATAREFERENCE_6fca592c224f449c8f06f51e2cfb74a9"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_on_datastore = 'my_dataset_for_batch_inference'\n",
    "datastore = Datastore.get(workspace=workspace, datastore_name='workspaceblobstore')\n",
    "datastore.upload(src_dir='data_for_batch_inference', target_path=path_on_datastore, overwrite=True, show_progress=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### register\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "{\n  \"source\": [\n    \"('workspaceblobstore', 'my_dataset_for_batch_inference')\"\n  ],\n  \"definition\": [\n    \"GetDatastoreFiles\"\n  ],\n  \"registration\": {\n    \"id\": \"6d939c01-f8a8-49f9-a233-cf9ce2c55ccf\",\n    \"name\": \"THUCNews_For_Batch_Inference\",\n    \"version\": 1,\n    \"description\": \"THUCNews dataset is generated by filtering and filtering historical data of Sina News RSS subscription channel from 2005 to 2011\",\n    \"workspace\": \"Workspace.create(name='fundamental3', subscription_id='4f455bd0-f95a-4b7d-8d08-078611508e0b', resource_group='fundamental')\"\n  }\n}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'THUCNews_For_Batch_Inference'\n",
    "description = 'THUCNews dataset is generated by filtering and filtering historical data \\\n",
    "of Sina News RSS subscription channel from 2005 to 2011'\n",
    "datastore_path = [DataPath(datastore=datastore, path_on_datastore=path_on_datastore)]\n",
    "data = Dataset.File.from_files(path=datastore_path)\n",
    "data.register(workspace=workspace, name=dataset_name, description=description, create_new_version=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}