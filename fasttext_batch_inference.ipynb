{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Using a Trained Model for Batch Inference\n",
    "\n",
    "In this notebook, we will demonstrate how to make predictions on large quantities of data asynchronously using the ML pipelines with Azure Machine Learning. Batch inference (or batch scoring) provides cost-effective inference, with unparalleled throughput for asynchronous applications. Batch prediction pipelines can scale to perform inference on terabytes of production data. Batch prediction is optimized for high throughput, fire-and-forget predictions for a large collection of data.\n",
    "\n",
    "> **Tip**\n",
    "The dataset we use is not that huge. We aim to make you know the workflow of batch inference. If your system requires low-latency processing (to process a single document or small set of documents quickly), please use realtime inference. Refer to fasttext_realtime_inference.ipynb for more details. \n",
    "\n",
    "The outline of this notebook is as follows:\n",
    "\n",
    "- Create a DataStore referencing documents stored in a blob container.\n",
    "- Reference a trained fastText model from a complete experiment.\n",
    "- Use the fastText model to do batch inference on the documents in the data blob container.\n",
    "\n",
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at https://github.com/Azure/MachineLearningNotebooks first. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.pipeline.wrapper import PipelineRun, Module, dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "Create a workspace object from the existing workspace. Workspace.from_config() reads the file config.json and loads the details into an object named workspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DesignerDRI_EASTUS\n",
      "DesignerDRI\n",
      "eastus\n",
      "74eccef0-4b8d-4f83-b5f9-fa100d155b22\n",
      "dict_keys(['aks-dev', 'aks-dev2', 'attached-aks', 'default', 'compute', 'cpu-cluster', 'aml-compute'])\n"
     ]
    }
   ],
   "source": [
    "workspace = Workspace.from_config('config.json')\n",
    "print(workspace.name, workspace.resource_group, workspace.location, workspace.subscription_id,\n",
    "      workspace.compute_targets.keys(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes. If the AmlCompute with that name is already in your workspace the code will skip the creation process.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target: aml-compute\n"
     ]
    }
   ],
   "source": [
    "aml_compute_name = 'aml-compute'\n",
    "if aml_compute_name in workspace.compute_targets:\n",
    "    aml_compute = AmlCompute(workspace, aml_compute_name)\n",
    "    print(\"Found existing compute target: {}\".format(aml_compute_name))\n",
    "else:\n",
    "    print(\"Creating new compute target: {}\".format(aml_compute_name))\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\", min_nodes=1, max_nodes=4)\n",
    "    aml_compute = ComputeTarget.create(workspace, aml_compute_name, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the dataset onto a blob container and register it to the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 200 files\n",
      "Uploading data_for_batch_inference/0\n",
      "Uploading data_for_batch_inference/1\n",
      "Uploading data_for_batch_inference/10\n",
      "Uploading data_for_batch_inference/100\n",
      "Uploading data_for_batch_inference/101\n",
      "Uploading data_for_batch_inference/102\n",
      "Uploading data_for_batch_inference/103\n",
      "Uploading data_for_batch_inference/104\n",
      "Uploading data_for_batch_inference/105\n",
      "Uploading data_for_batch_inference/106\n",
      "Uploading data_for_batch_inference/107\n",
      "Uploading data_for_batch_inference/108\n",
      "Uploading data_for_batch_inference/109\n",
      "Uploading data_for_batch_inference/11\n",
      "Uploading data_for_batch_inference/110\n",
      "Uploading data_for_batch_inference/111\n",
      "Uploading data_for_batch_inference/112\n",
      "Uploading data_for_batch_inference/113\n",
      "Uploading data_for_batch_inference/114\n",
      "Uploading data_for_batch_inference/115\n",
      "Uploading data_for_batch_inference/116\n",
      "Uploading data_for_batch_inference/117\n",
      "Uploading data_for_batch_inference/118\n",
      "Uploading data_for_batch_inference/119\n",
      "Uploading data_for_batch_inference/12\n",
      "Uploading data_for_batch_inference/120\n",
      "Uploading data_for_batch_inference/121\n",
      "Uploading data_for_batch_inference/122\n",
      "Uploading data_for_batch_inference/123\n",
      "Uploading data_for_batch_inference/124\n",
      "Uploading data_for_batch_inference/125\n",
      "Uploading data_for_batch_inference/126\n",
      "Uploaded data_for_batch_inference/110, 1 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/127\n",
      "Uploaded data_for_batch_inference/10, 2 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/128\n",
      "Uploaded data_for_batch_inference/123, 3 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/129\n",
      "Uploaded data_for_batch_inference/101, 4 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/13\n",
      "Uploaded data_for_batch_inference/120, 5 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/130\n",
      "Uploaded data_for_batch_inference/109, 6 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/131\n",
      "Uploaded data_for_batch_inference/122, 7 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/132\n",
      "Uploaded data_for_batch_inference/105, 8 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/133\n",
      "Uploaded data_for_batch_inference/11, 9 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/134\n",
      "Uploaded data_for_batch_inference/116, 10 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/135\n",
      "Uploaded data_for_batch_inference/112, 11 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/136\n",
      "Uploaded data_for_batch_inference/126, 12 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/137\n",
      "Uploaded data_for_batch_inference/100, 13 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/138\n",
      "Uploaded data_for_batch_inference/107, 14 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/139\n",
      "Uploaded data_for_batch_inference/12, 15 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/14\n",
      "Uploaded data_for_batch_inference/125, 16 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/140\n",
      "Uploaded data_for_batch_inference/106, 17 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/141\n",
      "Uploaded data_for_batch_inference/118, 18 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/114, 19 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/142\n",
      "Uploading data_for_batch_inference/143\n",
      "Uploaded data_for_batch_inference/1, 20 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/144\n",
      "Uploaded data_for_batch_inference/115, 21 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/145\n",
      "Uploaded data_for_batch_inference/0, 22 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/146\n",
      "Uploaded data_for_batch_inference/113, 23 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/147\n",
      "Uploaded data_for_batch_inference/117, 24 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/148\n",
      "Uploaded data_for_batch_inference/104, 25 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/149\n",
      "Uploaded data_for_batch_inference/111, 26 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/102, 27 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/15\n",
      "Uploading data_for_batch_inference/150\n",
      "Uploaded data_for_batch_inference/121, 28 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/151\n",
      "Uploaded data_for_batch_inference/108, 29 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/152\n",
      "Uploaded data_for_batch_inference/119, 30 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/153\n",
      "Uploaded data_for_batch_inference/103, 31 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/154\n",
      "Uploaded data_for_batch_inference/124, 32 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/155\n",
      "Uploaded data_for_batch_inference/127, 33 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/156\n",
      "Uploaded data_for_batch_inference/128, 34 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/157\n",
      "Uploaded data_for_batch_inference/129, 35 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/158\n",
      "Uploaded data_for_batch_inference/130, 36 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/159\n",
      "Uploaded data_for_batch_inference/131, 37 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/16\n",
      "Uploaded data_for_batch_inference/13, 38 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/160\n",
      "Uploaded data_for_batch_inference/132, 39 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/161\n",
      "Uploaded data_for_batch_inference/133, 40 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/162\n",
      "Uploaded data_for_batch_inference/134, 41 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/163\n",
      "Uploaded data_for_batch_inference/135, 42 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/164\n",
      "Uploaded data_for_batch_inference/136, 43 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/165\n",
      "Uploaded data_for_batch_inference/138, 44 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/139, 45 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/166\n",
      "Uploading data_for_batch_inference/167\n",
      "Uploaded data_for_batch_inference/14, 46 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/168\n",
      "Uploaded data_for_batch_inference/141, 47 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/169\n",
      "Uploaded data_for_batch_inference/140, 48 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/17\n",
      "Uploaded data_for_batch_inference/143, 49 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/142, 50 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/170\n",
      "Uploading data_for_batch_inference/171\n",
      "Uploaded data_for_batch_inference/144, 51 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/172\n",
      "Uploaded data_for_batch_inference/137, 52 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/173\n",
      "Uploaded data_for_batch_inference/146, 53 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/150, 54 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/174\n",
      "Uploading data_for_batch_inference/175\n",
      "Uploaded data_for_batch_inference/151, 55 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/147, 56 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/176\n",
      "Uploaded data_for_batch_inference/149, 57 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/177\n",
      "Uploaded data_for_batch_inference/152, 58 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/178\n",
      "Uploading data_for_batch_inference/179\n",
      "Uploaded data_for_batch_inference/145, 59 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/18\n",
      "Uploaded data_for_batch_inference/15, 60 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/154, 61 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/180\n",
      "Uploading data_for_batch_inference/181\n",
      "Uploaded data_for_batch_inference/148, 62 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/153, 63 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/182\n",
      "Uploading data_for_batch_inference/183\n",
      "Uploaded data_for_batch_inference/155, 64 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data_for_batch_inference/156, 65 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/185\n",
      "Uploaded data_for_batch_inference/157, 66 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/186\n",
      "Uploaded data_for_batch_inference/158, 67 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/187\n",
      "Uploaded data_for_batch_inference/160, 68 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/188\n",
      "Uploaded data_for_batch_inference/159, 69 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/189\n",
      "Uploaded data_for_batch_inference/16, 70 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/19\n",
      "Uploaded data_for_batch_inference/161, 71 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/190\n",
      "Uploaded data_for_batch_inference/162, 72 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/191\n",
      "Uploaded data_for_batch_inference/163, 73 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/192\n",
      "Uploaded data_for_batch_inference/164, 74 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/193\n",
      "Uploaded data_for_batch_inference/165, 75 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/194\n",
      "Uploaded data_for_batch_inference/168, 76 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/195\n",
      "Uploaded data_for_batch_inference/167, 77 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/196\n",
      "Uploaded data_for_batch_inference/166, 78 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/197\n",
      "Uploaded data_for_batch_inference/169, 79 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/198\n",
      "Uploaded data_for_batch_inference/170, 80 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/199\n",
      "Uploaded data_for_batch_inference/171, 81 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/2\n",
      "Uploaded data_for_batch_inference/17, 82 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/20\n",
      "Uploaded data_for_batch_inference/172, 83 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/21\n",
      "Uploaded data_for_batch_inference/173, 84 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/22\n",
      "Uploaded data_for_batch_inference/178, 85 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/23\n",
      "Uploaded data_for_batch_inference/181, 86 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/24\n",
      "Uploaded data_for_batch_inference/18, 87 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/25\n",
      "Uploaded data_for_batch_inference/177, 88 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/26\n",
      "Uploaded data_for_batch_inference/176, 89 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/27\n",
      "Uploaded data_for_batch_inference/179, 90 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/28\n",
      "Uploaded data_for_batch_inference/182, 91 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/29\n",
      "Uploaded data_for_batch_inference/174, 92 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/180, 93 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/3\n",
      "Uploading data_for_batch_inference/30\n",
      "Uploaded data_for_batch_inference/184, 94 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/31\n",
      "Uploaded data_for_batch_inference/183, 95 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/32\n",
      "Uploaded data_for_batch_inference/175, 96 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/33\n",
      "Uploaded data_for_batch_inference/185, 97 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/34\n",
      "Uploaded data_for_batch_inference/186, 98 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/35\n",
      "Uploaded data_for_batch_inference/187, 99 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/36\n",
      "Uploaded data_for_batch_inference/188, 100 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/37\n",
      "Uploaded data_for_batch_inference/19, 101 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/38\n",
      "Uploaded data_for_batch_inference/189, 102 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/39\n",
      "Uploaded data_for_batch_inference/190, 103 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/191, 104 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/4\n",
      "Uploading data_for_batch_inference/40\n",
      "Uploaded data_for_batch_inference/192, 105 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/41\n",
      "Uploaded data_for_batch_inference/193, 106 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/42\n",
      "Uploaded data_for_batch_inference/195, 107 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/43\n",
      "Uploaded data_for_batch_inference/196, 108 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/194, 109 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/44\n",
      "Uploading data_for_batch_inference/45\n",
      "Uploaded data_for_batch_inference/197, 110 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/46\n",
      "Uploaded data_for_batch_inference/198, 111 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/47\n",
      "Uploaded data_for_batch_inference/199, 112 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/48\n",
      "Uploaded data_for_batch_inference/20, 113 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/2, 114 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/49\n",
      "Uploading data_for_batch_inference/5\n",
      "Uploaded data_for_batch_inference/22, 115 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/50\n",
      "Uploaded data_for_batch_inference/23, 116 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/51\n",
      "Uploaded data_for_batch_inference/27, 117 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/52\n",
      "Uploaded data_for_batch_inference/28, 118 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/53\n",
      "Uploaded data_for_batch_inference/24, 119 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/54\n",
      "Uploaded data_for_batch_inference/21, 120 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/55\n",
      "Uploaded data_for_batch_inference/26, 121 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/25, 122 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/29, 123 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/56\n",
      "Uploaded data_for_batch_inference/31, 124 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/57\n",
      "Uploading data_for_batch_inference/58\n",
      "Uploaded data_for_batch_inference/3, 125 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/59\n",
      "Uploading data_for_batch_inference/6\n",
      "Uploaded data_for_batch_inference/32, 126 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/33, 127 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/60\n",
      "Uploading data_for_batch_inference/61\n",
      "Uploaded data_for_batch_inference/30, 128 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/62\n",
      "Uploaded data_for_batch_inference/34, 129 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/63\n",
      "Uploaded data_for_batch_inference/35, 130 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/64\n",
      "Uploaded data_for_batch_inference/36, 131 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/65\n",
      "Uploaded data_for_batch_inference/37, 132 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/66\n",
      "Uploaded data_for_batch_inference/38, 133 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/67\n",
      "Uploaded data_for_batch_inference/4, 134 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/39, 135 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/68\n",
      "Uploading data_for_batch_inference/69\n",
      "Uploaded data_for_batch_inference/40, 136 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/7\n",
      "Uploaded data_for_batch_inference/41, 137 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/70\n",
      "Uploaded data_for_batch_inference/42, 138 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/71\n",
      "Uploaded data_for_batch_inference/43, 139 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/72\n",
      "Uploaded data_for_batch_inference/45, 140 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/46, 141 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/73\n",
      "Uploading data_for_batch_inference/74\n",
      "Uploaded data_for_batch_inference/44, 142 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/75\n",
      "Uploaded data_for_batch_inference/47, 143 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/76\n",
      "Uploaded data_for_batch_inference/48, 144 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/77\n",
      "Uploaded data_for_batch_inference/49, 145 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/78\n",
      "Uploaded data_for_batch_inference/5, 146 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/79\n",
      "Uploaded data_for_batch_inference/51, 147 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/58, 148 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/8\n",
      "Uploading data_for_batch_inference/80\n",
      "Uploaded data_for_batch_inference/54, 149 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/81\n",
      "Uploaded data_for_batch_inference/50, 150 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/82\n",
      "Uploaded data_for_batch_inference/6, 151 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/83\n",
      "Uploaded data_for_batch_inference/56, 152 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/57, 153 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/84\n",
      "Uploading data_for_batch_inference/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data_for_batch_inference/53, 154 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/86\n",
      "Uploaded data_for_batch_inference/55, 155 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/87\n",
      "Uploaded data_for_batch_inference/52, 156 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/88\n",
      "Uploaded data_for_batch_inference/61, 157 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/89\n",
      "Uploaded data_for_batch_inference/59, 158 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/9\n",
      "Uploaded data_for_batch_inference/62, 159 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/90\n",
      "Uploaded data_for_batch_inference/60, 160 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/91\n",
      "Uploaded data_for_batch_inference/63, 161 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/92\n",
      "Uploaded data_for_batch_inference/64, 162 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/93\n",
      "Uploaded data_for_batch_inference/65, 163 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/94\n",
      "Uploaded data_for_batch_inference/67, 164 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/66, 165 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/95\n",
      "Uploading data_for_batch_inference/96\n",
      "Uploaded data_for_batch_inference/69, 166 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/68, 167 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/97\n",
      "Uploading data_for_batch_inference/98\n",
      "Uploaded data_for_batch_inference/7, 168 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/99\n",
      "Uploaded data_for_batch_inference/70, 169 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/71, 170 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/72, 171 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/73, 172 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/74, 173 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/75, 174 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/76, 175 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/78, 176 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/77, 177 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/79, 178 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/85, 179 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/83, 180 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/82, 181 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/8, 182 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/84, 183 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/89, 184 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/80, 185 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/9, 186 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/87, 187 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/90, 188 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/86, 189 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/81, 190 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/88, 191 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/91, 192 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/92, 193 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/93, 194 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/96, 195 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/95, 196 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/94, 197 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/97, 198 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/99, 199 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/98, 200 files out of an estimated total of 200\n",
      "Uploaded 200 files\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'THUCNews_For_Batch_Inference'\n",
    "# if the workspace don't contain the dataset, then register it\n",
    "if not dataset_name in workspace.datasets:\n",
    "    # upload files onto path_on_datastore to a blob container\n",
    "    # our files are in the directory of 'path_on_datastore' in the blob container\n",
    "    path_on_datastore = 'data_for_batch_inference'\n",
    "    datastore = Datastore.get(workspace=workspace, datastore_name='workspaceblobstore')\n",
    "    datastore.upload(src_dir='data_for_batch_inference', target_path=path_on_datastore, overwrite=True, show_progress=True)\n",
    "    # description of the dataset\n",
    "    description = 'THUCNews dataset is generated by filtering and filtering historical data \\\n",
    "    of Sina News RSS subscription channel from 2005 to 2011'\n",
    "    # get the DataPath object associated with the uploaded dataset\n",
    "    datastore_path = [DataPath(datastore=datastore, path_on_datastore=path_on_datastore)]\n",
    "    data = Dataset.File.from_files(path=datastore_path)\n",
    "    # register the dataset to your workspace\n",
    "    data.register(workspace=workspace, name=dataset_name, description=description, create_new_version=True)\n",
    "# get the registered dataset\n",
    "dataset = workspace.datasets[dataset_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register an anonymous module from yaml file to the workspace.\n",
    "If you decorate your module function with ```@dsl.module```, azure-cli-ml could help to generate the ```*.spec.yaml``` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fasttext_score_module_func = Module.from_yaml(workspace, 'fasttext_score/fasttext_score.spec.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a trained fastText model from a complete experiment\n",
    "- get all experiments\n",
    "- choose an experiment from all experiments\n",
    "- get the latest run\n",
    "- get a PipelineRun associated with the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample10',\n",
       " 'sample5',\n",
       " 'sample5-realtime',\n",
       " 'simple10-batch',\n",
       " 'pythonscript',\n",
       " 'Data_dependency',\n",
       " 'clement',\n",
       " 'new_module',\n",
       " 'test_module2',\n",
       " 'test_m',\n",
       " 'module_SDK_local_module_test',\n",
       " 'fasttext_pipeline']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name_list = [exp.name for exp in Experiment.list(workspace)]\n",
    "exp_name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the experiment you want with its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Workspace</th><th>Report Page</th><th>Docs Page</th></tr><tr><td>fasttext_pipeline</td><td>DesignerDRI_EASTUS</td><td><a href=\"https://ml.azure.com/experiments/fasttext_pipeline?wsid=/subscriptions/74eccef0-4b8d-4f83-b5f9-fa100d155b22/resourcegroups/DesignerDRI/workspaces/DesignerDRI_EASTUS\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.Experiment?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Experiment(Name: fasttext_pipeline,\n",
       "Workspace: DesignerDRI_EASTUS)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = \"fasttext_pipeline\"\n",
    "experiment = Experiment(workspace, experiment_name)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>fasttext_pipeline</td><td>5e91f20b-c728-45c5-932e-4c5d9913a067</td><td>azureml.PipelineRun</td><td>Completed</td><td><a href=\"https://ml.azure.com/experiments/fasttext_pipeline/runs/5e91f20b-c728-45c5-932e-4c5d9913a067?wsid=/subscriptions/74eccef0-4b8d-4f83-b5f9-fa100d155b22/resourcegroups/DesignerDRI/workspaces/DesignerDRI_EASTUS\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: fasttext_pipeline,\n",
       "Id: 5e91f20b-c728-45c5-932e-4c5d9913a067,\n",
       "Type: azureml.PipelineRun,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# azureml.pipeline.core.run.PipelineRun\n",
    "run = experiment.get_runs().__next__()\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a PipelineRun object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>fasttext_pipeline</td><td>5e91f20b-c728-45c5-932e-4c5d9913a067</td><td>azureml.PipelineRun</td><td>Completed</td><td><a href=\"https://ml.azure.com/experiments/fasttext_pipeline/runs/5e91f20b-c728-45c5-932e-4c5d9913a067?wsid=/subscriptions/74eccef0-4b8d-4f83-b5f9-fa100d155b22/resourcegroups/DesignerDRI/workspaces/DesignerDRI_EASTUS\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: fasttext_pipeline,\n",
       "Id: 5e91f20b-c728-45c5-932e-4c5d9913a067,\n",
       "Type: azureml.PipelineRun,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = run.id\n",
    "# azureml.pipeline.wrapper._pipeline_run.PipelineRun\n",
    "pipeline_run = PipelineRun(experiment, run_id)\n",
    "pipeline_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the pipeline so as to obtain information about the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the trained model from a StepRun object.\n",
    "- get a StepRun from the PipelineRun\n",
    "- get the port with the trained model from the StepRun\n",
    "- get DataPath from the port\n",
    "- change DataPath into the form of module input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>fasttext_pipeline</td><td>4d746d10-89da-4e9c-81f7-7b4b99d405fd</td><td>azureml.StepRun</td><td>Completed</td><td><a href=\"https://ml.azure.com/experiments/fasttext_pipeline/runs/4d746d10-89da-4e9c-81f7-7b4b99d405fd?wsid=/subscriptions/74eccef0-4b8d-4f83-b5f9-fa100d155b22/resourcegroups/DesignerDRI/workspaces/DesignerDRI_EASTUS\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "StepRun(Experiment: fasttext_pipeline,\n",
       "Id: 4d746d10-89da-4e9c-81f7-7b4b99d405fd,\n",
       "Type: azureml.StepRun,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step_run_id will be from the visualization result.\n",
    "step_run_id = '4d746d10-89da-4e9c-81f7-7b4b99d405fd'\n",
    "step_run = pipeline_run.get_step_run(step_run_id)\n",
    "step_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the trained model as the input of a new pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# name will be from the visualization result.\n",
    "# get_port() should supports three kinds of names: (1)the_better_model (2)The better model (3)The_better_model\n",
    "port = step_run.get_port(name='The_better_model')\n",
    "data_path = port.get_data_path()\n",
    "model = Dataset.File.from_files(path=[data_path]).as_named_input('model_for_batch_inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name='batch inference', description='Batch Inference', default_compute_target=aml_compute.name)\n",
    "def training_pipeline():\n",
    "    fasttext_score = fasttext_score_module_func(\n",
    "        texts_to_score=dataset,\n",
    "        fasttext_model_dir=model\n",
    "    )\n",
    "    fasttext_score.runsettings.configure(node_count=2, process_count_per_node=2, mini_batch_size=\"64\")\n",
    "\n",
    "    return {**fasttext_score.outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pipeline\n",
    "pipeline = training_pipeline()\n",
    "# pipeline.save(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "try {\n",
       "    require.undef(\"validate_widget\")\n",
       "\n",
       "    define('validate_widget', [\"@jupyter-widgets/base\"], function(widgets) {\n",
       "        var ValidateView = widgets.DOMWidgetView.extend({\n",
       "            render () {\n",
       "                window.widget_self = this\n",
       "                var visualize_id = this.model.get('visualize_id')\n",
       "\n",
       "                if (!window._renderLock) {\n",
       "                    window._renderLock = {}\n",
       "                }\n",
       "                if (window._renderLock[visualize_id]) {\n",
       "                    return\n",
       "                }\n",
       "                window._renderLock[visualize_id] = \"widget\"\n",
       "                console.log(\"load as widget\", Date.now())\n",
       "\n",
       "                var lib_url = this.model.get('lib_url')\n",
       "                var graph_json = JSON.parse(this.model.get('graph_json'))\n",
       "                var env_json = JSON.parse(this.model.get('env_json'))\n",
       "                var container_id = this.model.get('container_id')\n",
       "\n",
       "                window.render_container_id = container_id\n",
       "                window.graph_json = graph_json\n",
       "                window.graph_json_to_compare = undefined\n",
       "                window.env_json = env_json\n",
       "                window.before_script = performance.now()\n",
       "\n",
       "                var container = document.createElement('div')\n",
       "                container.id = container_id\n",
       "                this.el.appendChild(container)\n",
       "\n",
       "                var style = document.createElement('style')\n",
       "                style.innerHTML = [\n",
       "                    \"#\", container_id, \" svg.react-dag-editor-svg-container { height: 800px; }\",\n",
       "                    \".cell-output-ipywidget-background { background: transparent !important }\"\n",
       "                ].join('')\n",
       "                this.el.appendChild(style)\n",
       "\n",
       "                this.model.on('msg:custom', dispatchMessage, this);\n",
       "\n",
       "                if (!window.__event_hub) {\n",
       "                    window.__event_hub = {}\n",
       "                }\n",
       "                if (!window.__event_hub[container_id]) {\n",
       "                    window.__event_hub[container_id] = {}\n",
       "                }\n",
       "\n",
       "                if (!window.__send_event) {\n",
       "                    window.__send_event = {}\n",
       "                }\n",
       "                window.__send_event[container_id] = sendMessage.bind(this)\n",
       "\n",
       "                function sendMessage(message, uid, content) {\n",
       "                    return new Promise((resolve) => {\n",
       "                        this.model.send({\n",
       "                            message: `${message}:request`,\n",
       "                            body: {\n",
       "                                uid,\n",
       "                                content\n",
       "                            }\n",
       "                        })\n",
       "    \n",
       "                        var respMessageKey = `${message}:response`\n",
       "                        if (!window.__event_hub[container_id][respMessageKey]) {\n",
       "                            window.__event_hub[container_id][respMessageKey] = []\n",
       "                        }\n",
       "                        window.__event_hub[container_id][respMessageKey].push(callback)\n",
       "    \n",
       "                        function callback (response) {\n",
       "                            if (response.uid !== uid) {\n",
       "                                return\n",
       "                            }\n",
       "\n",
       "                            var idx = window.__event_hub[container_id][respMessageKey].indexOf(callback) \n",
       "                            window.__event_hub[container_id][respMessageKey].splice(idx, 1)\n",
       "                            \n",
       "                            resolve(response)\n",
       "                        }\n",
       "                    })\n",
       "                }\n",
       "\n",
       "                function dispatchMessage (rawMessage) {\n",
       "                    var message = rawMessage.message\n",
       "                    var body = rawMessage.body\n",
       "\n",
       "                    if (!window.__event_hub[container_id][message]) {\n",
       "                        window.__event_hub[container_id][message] = []\n",
       "                    }\n",
       "                    var listeners = window.__event_hub[container_id][message]\n",
       "\n",
       "                    listeners.forEach(cb => {\n",
       "                        try {\n",
       "                            cb(body)\n",
       "                        } catch (e) {\n",
       "                            console.error(\"Unexpected error in listener\", e)\n",
       "                        }\n",
       "                    })\n",
       "\n",
       "                    console.log(body)\n",
       "                }\n",
       "\n",
       "                var script = document.createElement('script')\n",
       "                script.src = lib_url\n",
       "                this.el.appendChild(script)\n",
       "            }\n",
       "        });\n",
       "\n",
       "        return {\n",
       "            ValidateView\n",
       "        }\n",
       "    })\n",
       "} catch (e) {\n",
       "    console.log(\"create validation widget failed\", e)\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032a29280a9b4d0789f28a2f67b23121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ValidateView(container_id='container_id_a323ef3a-a98b-4028-b833-018f8e99fef4_widget', env_json='{\"subscription"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "        #container_id_a323ef3a-a98b-4028-b833-018f8e99fef4_script svg.react-dag-editor-svg-container {\n",
       "            height: 800px;\n",
       "        }\n",
       "        </style>\n",
       "        <div id=\"container_id_a323ef3a-a98b-4028-b833-018f8e99fef4_script\"></div>\n",
       "        <script>\n",
       "            (function () {\n",
       "                if (!window._renderLock) {\n",
       "                    window._renderLock = {}\n",
       "                }\n",
       "                if (window._renderLock[\"a323ef3a-a98b-4028-b833-018f8e99fef4\"]) {\n",
       "                    return\n",
       "                }\n",
       "                window._renderLock[\"a323ef3a-a98b-4028-b833-018f8e99fef4\"] = \"script\"\n",
       "                console.log(\"load as script\", Date.now())\n",
       "                window.render_container_id=\"container_id_a323ef3a-a98b-4028-b833-018f8e99fef4_script\";\n",
       "                window.graph_json={\"pipeline\": {\"name\": [\"batch inference\"], \"data_references\": {\"THUCNews_For_Batch_Inference\": {\"dataset_id\": \"0e6ad64e-9648-450a-b103-b3ea3ef44f78\"}, \"490f4dd7-fe83-4dde-8c47-d52e7f597c62\": {\"saved_id\": \"490f4dd7-fe83-4dde-8c47-d52e7f597c62\"}}, \"steps\": {\"5e9dde1b\": {\"inputs\": {\"Texts_to_score\": {\"source\": \"THUCNews_For_Batch_Inference\"}, \"Fasttext_model_dir\": {\"source\": \"490f4dd7-fe83-4dde-8c47-d52e7f597c62\"}}, \"outputs\": {\"Scored_data_output_dir\": {\"destination\": \"f2cec2ad-9699-48ec-a0ef-57a3d41ea87b_Scored_data_output_dir\"}}, \"module\": {\"id\": \"98a3aebb-8ff6-48d9-8f1d-2377bb2cd85d\", \"version\": \"0.0.22\"}, \"parameters\": {}, \"validate\": {\"error\": [], \"module_id\": \"98a3aebb-8ff6-48d9-8f1d-2377bb2cd85d\", \"namespace\": \"DesignerDRI_EASTUS\", \"module_name\": \"FastText Score\", \"module_version\": \"0.0.22\"}}}}, \"modules\": [{\"module_id\": \"98a3aebb-8ff6-48d9-8f1d-2377bb2cd85d\", \"version\": \"0.0.22\", \"name\": \"FastText Score\", \"namespace\": \"DesignerDRI_EASTUS\", \"structured_interface\": {\"inputs\": [{\"name\": \"Texts_to_score\", \"label\": \"Texts to score\", \"description\": null, \"data_type_ids_list\": [\"AnyDirectory\"]}, {\"name\": \"Fasttext_model_dir\", \"label\": \"Fasttext model dir\", \"description\": null, \"data_type_ids_list\": [\"AnyDirectory\"]}], \"outputs\": [{\"name\": \"Scored_data_output_dir\", \"label\": \"Scored data output dir\", \"description\": null, \"data_type_id\": \"AnyDirectory\"}]}}], \"datasources\": [{\"name\": \"THUCNews_For_Batch_Inference\", \"description\": \"THUCNews dataset is generated by filtering and filtering historical data     of Sina News RSS subscription channel from 2005 to 2011\", \"version\": 1, \"tags\": {}, \"registered_id\": \"0e6ad64e-9648-450a-b103-b3ea3ef44f78\", \"saved_id\": \"6709d320-3ff8-4c69-9d62-b36311d71072\", \"nodeId\": \"5d50b1c2-6cc3-36d4-b05d-24fc47ae7db6\"}, {\"name\": \"model_for_batch_inference\", \"mode\": \"direct\", \"saved_id\": \"490f4dd7-fe83-4dde-8c47-d52e7f597c62\", \"nodeId\": \"537b2f5a-5fda-322f-9101-f130b918435d\"}], \"subGraphInfo\": [{\"name\": \"batch inference\", \"description\": \"Batch Inference\", \"default_compute_target\": {\"name\": \"aml-compute\"}, \"default_data_store\": {\"data_store_name\": \"workspaceblobstore\"}, \"id\": \"1177cb37-6724-417c-8663-a837c21b98a6\", \"pipeline_definition_id\": \"e730b425-7b64-4b42-af9a-3b0c06aa058b\", \"sub_graph_parameter_assignment\": [], \"sub_graph_data_path_parameter_assignment\": [], \"sub_graph_default_compute_target_nodes\": [\"5e9dde1b\"], \"sub_graph_default_data_store_nodes\": [\"5e9dde1b\"], \"inputs\": [], \"outputs\": [{\"name\": \"scored_data_output_dir\", \"internal\": [{\"node_id\": \"5e9dde1b\", \"port_name\": \"Scored_data_output_dir\"}], \"external\": []}]}], \"nodeIdToSubGraphIdMapping\": {\"5e9dde1b\": \"1177cb37-6724-417c-8663-a837c21b98a6\"}, \"subPipelineDefinition\": [{\"name\": \"batch inference\", \"description\": \"Batch Inference\", \"default_compute_target\": {\"name\": \"aml-compute\"}, \"default_data_store\": {\"data_store_name\": \"workspaceblobstore\"}, \"pipeline_function_name\": \"training_pipeline\", \"id\": \"e730b425-7b64-4b42-af9a-3b0c06aa058b\", \"from_module_name\": \"__main__\", \"parameter_list\": []}]};\n",
       "                window.graph_json_to_compare=undefined;\n",
       "                window.env_json={\"subscription_id\": \"74eccef0-4b8d-4f83-b5f9-fa100d155b22\"};\n",
       "                window.before_script = performance.now();\n",
       "                var script = document.createElement('script')\n",
       "                script.src = \"https://yucongj.azureedge.net/libs/prod/0.0.8/index.js\"\n",
       "                document.getElementById(\"container_id_a323ef3a-a98b-4028-b833-018f8e99fef4_script\").appendChild(script)\n",
       "            })()\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'result': 'validation passed', 'errors': []}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted PipelineRun 869845ff-8b4b-4bea-88c5-da49d2db2f43\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/fasttext_batch_inference/runs/869845ff-8b4b-4bea-88c5-da49d2db2f43?wsid=/subscriptions/74eccef0-4b8d-4f83-b5f9-fa100d155b22/resourcegroups/DesignerDRI/workspaces/DesignerDRI_EASTUS\n",
      "PipelineRunId: 869845ff-8b4b-4bea-88c5-da49d2db2f43\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/fasttext_batch_inference/runs/869845ff-8b4b-4bea-88c5-da49d2db2f43?wsid=/subscriptions/74eccef0-4b8d-4f83-b5f9-fa100d155b22/resourcegroups/DesignerDRI/workspaces/DesignerDRI_EASTUS\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "try {\n",
       "    require.undef(\"validate_widget\")\n",
       "\n",
       "    define('validate_widget', [\"@jupyter-widgets/base\"], function(widgets) {\n",
       "        var ValidateView = widgets.DOMWidgetView.extend({\n",
       "            render () {\n",
       "                window.widget_self = this\n",
       "                var visualize_id = this.model.get('visualize_id')\n",
       "\n",
       "                if (!window._renderLock) {\n",
       "                    window._renderLock = {}\n",
       "                }\n",
       "                if (window._renderLock[visualize_id]) {\n",
       "                    return\n",
       "                }\n",
       "                window._renderLock[visualize_id] = \"widget\"\n",
       "                console.log(\"load as widget\", Date.now())\n",
       "\n",
       "                var lib_url = this.model.get('lib_url')\n",
       "                var graph_json = JSON.parse(this.model.get('graph_json'))\n",
       "                var env_json = JSON.parse(this.model.get('env_json'))\n",
       "                var container_id = this.model.get('container_id')\n",
       "\n",
       "                window.render_container_id = container_id\n",
       "                window.graph_json = graph_json\n",
       "                window.graph_json_to_compare = undefined\n",
       "                window.env_json = env_json\n",
       "                window.before_script = performance.now()\n",
       "\n",
       "                var container = document.createElement('div')\n",
       "                container.id = container_id\n",
       "                this.el.appendChild(container)\n",
       "\n",
       "                var style = document.createElement('style')\n",
       "                style.innerHTML = [\n",
       "                    \"#\", container_id, \" svg.react-dag-editor-svg-container { height: 800px; }\",\n",
       "                    \".cell-output-ipywidget-background { background: transparent !important }\"\n",
       "                ].join('')\n",
       "                this.el.appendChild(style)\n",
       "\n",
       "                this.model.on('msg:custom', dispatchMessage, this);\n",
       "\n",
       "                if (!window.__event_hub) {\n",
       "                    window.__event_hub = {}\n",
       "                }\n",
       "                if (!window.__event_hub[container_id]) {\n",
       "                    window.__event_hub[container_id] = {}\n",
       "                }\n",
       "\n",
       "                if (!window.__send_event) {\n",
       "                    window.__send_event = {}\n",
       "                }\n",
       "                window.__send_event[container_id] = sendMessage.bind(this)\n",
       "\n",
       "                function sendMessage(message, uid, content) {\n",
       "                    return new Promise((resolve) => {\n",
       "                        this.model.send({\n",
       "                            message: `${message}:request`,\n",
       "                            body: {\n",
       "                                uid,\n",
       "                                content\n",
       "                            }\n",
       "                        })\n",
       "    \n",
       "                        var respMessageKey = `${message}:response`\n",
       "                        if (!window.__event_hub[container_id][respMessageKey]) {\n",
       "                            window.__event_hub[container_id][respMessageKey] = []\n",
       "                        }\n",
       "                        window.__event_hub[container_id][respMessageKey].push(callback)\n",
       "    \n",
       "                        function callback (response) {\n",
       "                            if (response.uid !== uid) {\n",
       "                                return\n",
       "                            }\n",
       "\n",
       "                            var idx = window.__event_hub[container_id][respMessageKey].indexOf(callback) \n",
       "                            window.__event_hub[container_id][respMessageKey].splice(idx, 1)\n",
       "                            \n",
       "                            resolve(response)\n",
       "                        }\n",
       "                    })\n",
       "                }\n",
       "\n",
       "                function dispatchMessage (rawMessage) {\n",
       "                    var message = rawMessage.message\n",
       "                    var body = rawMessage.body\n",
       "\n",
       "                    if (!window.__event_hub[container_id][message]) {\n",
       "                        window.__event_hub[container_id][message] = []\n",
       "                    }\n",
       "                    var listeners = window.__event_hub[container_id][message]\n",
       "\n",
       "                    listeners.forEach(cb => {\n",
       "                        try {\n",
       "                            cb(body)\n",
       "                        } catch (e) {\n",
       "                            console.error(\"Unexpected error in listener\", e)\n",
       "                        }\n",
       "                    })\n",
       "\n",
       "                    console.log(body)\n",
       "                }\n",
       "\n",
       "                var script = document.createElement('script')\n",
       "                script.src = lib_url\n",
       "                this.el.appendChild(script)\n",
       "            }\n",
       "        });\n",
       "\n",
       "        return {\n",
       "            ValidateView\n",
       "        }\n",
       "    })\n",
       "} catch (e) {\n",
       "    console.log(\"create validation widget failed\", e)\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24b030975234ed3bcd48497528bdf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ValidateView(container_id='container_id_de3864c1-8db6-4a70-b9dd-e2c1700f4150_widget', env_json='{}', graph_jso"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "        #container_id_de3864c1-8db6-4a70-b9dd-e2c1700f4150_script svg.react-dag-editor-svg-container {\n",
       "            height: 800px;\n",
       "        }\n",
       "        </style>\n",
       "        <div id=\"container_id_de3864c1-8db6-4a70-b9dd-e2c1700f4150_script\"></div>\n",
       "        <script>\n",
       "            (function () {\n",
       "                if (!window._renderLock) {\n",
       "                    window._renderLock = {}\n",
       "                }\n",
       "                if (window._renderLock[\"de3864c1-8db6-4a70-b9dd-e2c1700f4150\"]) {\n",
       "                    return\n",
       "                }\n",
       "                window._renderLock[\"de3864c1-8db6-4a70-b9dd-e2c1700f4150\"] = \"script\"\n",
       "                console.log(\"load as script\", Date.now())\n",
       "                window.render_container_id=\"container_id_de3864c1-8db6-4a70-b9dd-e2c1700f4150_script\";\n",
       "                window.graph_json={\"pipeline\": {\"name\": [\"batch inference\"], \"data_references\": {\"THUCNews_For_Batch_Inference\": {\"dataset_id\": \"0e6ad64e-9648-450a-b103-b3ea3ef44f78\"}, \"490f4dd7-fe83-4dde-8c47-d52e7f597c62\": {\"saved_id\": \"490f4dd7-fe83-4dde-8c47-d52e7f597c62\"}}, \"steps\": {\"0e554c2a\": {\"inputs\": {\"Texts_to_score\": {\"source\": \"THUCNews_For_Batch_Inference\"}, \"Fasttext_model_dir\": {\"source\": \"490f4dd7-fe83-4dde-8c47-d52e7f597c62\"}}, \"outputs\": {\"Scored_data_output_dir\": {\"destination\": \"f2cec2ad-9699-48ec-a0ef-57a3d41ea87b_Scored_data_output_dir\"}}, \"module\": {\"id\": \"98a3aebb-8ff6-48d9-8f1d-2377bb2cd85d\", \"version\": \"0.0.22\"}, \"parameters\": {}, \"validate\": {\"error\": [], \"module_id\": \"98a3aebb-8ff6-48d9-8f1d-2377bb2cd85d\", \"namespace\": \"DesignerDRI_EASTUS\", \"module_name\": \"FastText Score\", \"module_version\": \"0.0.22\"}}}}, \"modules\": [{\"module_id\": \"98a3aebb-8ff6-48d9-8f1d-2377bb2cd85d\", \"version\": \"0.0.22\", \"name\": \"FastText Score\", \"namespace\": \"DesignerDRI_EASTUS\", \"structured_interface\": {\"inputs\": [{\"name\": \"Texts_to_score\", \"label\": \"Texts to score\", \"description\": null, \"data_type_ids_list\": [\"AnyDirectory\"]}, {\"name\": \"Fasttext_model_dir\", \"label\": \"Fasttext model dir\", \"description\": null, \"data_type_ids_list\": [\"AnyDirectory\"]}], \"outputs\": [{\"name\": \"Scored_data_output_dir\", \"label\": \"Scored data output dir\", \"description\": null, \"data_type_id\": \"AnyDirectory\"}]}}], \"datasources\": [{\"name\": \"THUCNews_For_Batch_Inference\", \"description\": \"THUCNews dataset is generated by filtering and filtering historical data     of Sina News RSS subscription channel from 2005 to 2011\", \"version\": 1, \"tags\": {}, \"registered_id\": \"0e6ad64e-9648-450a-b103-b3ea3ef44f78\", \"saved_id\": \"6709d320-3ff8-4c69-9d62-b36311d71072\", \"nodeId\": \"5d50b1c2-6cc3-36d4-b05d-24fc47ae7db6\"}, {\"name\": \"model_for_batch_inference\", \"mode\": \"direct\", \"saved_id\": \"490f4dd7-fe83-4dde-8c47-d52e7f597c62\", \"nodeId\": \"537b2f5a-5fda-322f-9101-f130b918435d\"}], \"subGraphInfo\": [{\"name\": \"batch inference\", \"description\": \"Batch Inference\", \"default_compute_target\": {\"name\": \"aml-compute\"}, \"default_data_store\": {\"data_store_name\": \"workspaceblobstore\"}, \"id\": \"1177cb37-6724-417c-8663-a837c21b98a6\", \"pipeline_definition_id\": \"e730b425-7b64-4b42-af9a-3b0c06aa058b\", \"sub_graph_parameter_assignment\": [], \"sub_graph_data_path_parameter_assignment\": [], \"sub_graph_default_compute_target_nodes\": [\"0e554c2a\"], \"sub_graph_default_data_store_nodes\": [\"0e554c2a\"], \"inputs\": [], \"outputs\": [{\"name\": \"scored_data_output_dir\", \"internal\": [{\"node_id\": \"0e554c2a\", \"port_name\": \"Scored_data_output_dir\"}], \"external\": []}]}], \"nodeIdToSubGraphIdMapping\": {\"0e554c2a\": \"1177cb37-6724-417c-8663-a837c21b98a6\"}, \"subPipelineDefinition\": [{\"name\": \"batch inference\", \"description\": \"Batch Inference\", \"default_compute_target\": {\"name\": \"aml-compute\"}, \"default_data_store\": {\"data_store_name\": \"workspaceblobstore\"}, \"pipeline_function_name\": \"training_pipeline\", \"id\": \"e730b425-7b64-4b42-af9a-3b0c06aa058b\", \"from_module_name\": \"__main__\", \"parameter_list\": []}]};\n",
       "                window.graph_json_to_compare=undefined;\n",
       "                window.env_json={};\n",
       "                window.before_script = performance.now();\n",
       "                var script = document.createElement('script')\n",
       "                script.src = \"https://yucongj.azureedge.net/libs/prod/0.0.8/index.js\"\n",
       "                document.getElementById(\"container_id_de3864c1-8db6-4a70-b9dd-e2c1700f4150_script\").appendChild(script)\n",
       "            })()\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<RunStatus.completed: 'Completed'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline_run\n",
    "experiment_name = 'fasttext_batch_inference'\n",
    "pipeline_run = pipeline.submit(experiment_name=experiment_name, regenerate_outputs=True)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
